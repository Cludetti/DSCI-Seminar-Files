{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cludetti/DSCI-Seminar-Files/blob/main/Data_Science_for_Customer_Insights_Python_GRADED_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fCEDCU_qrC0"
      },
      "source": [
        "#Data Science for Customer Insights - Datascience Introduction\n",
        "\n",
        "Welcome to Data Science for Customer Insights. In the following tasks, you will learn how to do Data Science with Python. In the end, you will be able to:\n",
        "\n",
        "- do basic coding and use Python loops, lists, functions and more\n",
        "- access and apply machine learning algorithms to data sets\n",
        "- understand fundamental machine learning concepts\n",
        "\n",
        "Please go trough all tasks and answer the questions carefully. **This is task-based learning - you will have to Google a lot**. You can use any credible source material, you like, e.g.:\n",
        "\n",
        "- stackoverflow\n",
        "- SoloLearn\n",
        "- DataCamp\n",
        "- udemy\n",
        "\n",
        "Again - you will potentially have to Google everything, this is normal!\n",
        "\n",
        "Also, every time you see those signs \"**...**\" you have to change them to generate the correct code.\n",
        "\n",
        "**Note: This is the Notebook that part of your grade in the Seminar will be based on.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQWo_jfy4H9s"
      },
      "source": [
        "**Before you start: Please provide your information**\n",
        "\n",
        "Name:\n",
        "\n",
        "Matr.:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basics of Python programming\n",
        "The following cells will ask you about some of the most important concepts of Python programming. **If the questions in this section feel quite challenging to you, feel free to work through the Python Introduction Notebook first, to get familiar with Python programming.**\n",
        "\n",
        "Link to the Python Introduction Notebook: https://github.com/Maximilianwte/Data-Science-for-Customer-Insight/blob/main/Python%20Introduction%20-%20DSCI.ipynb (will not be graded)"
      ],
      "metadata": {
        "id": "jtbk80g3cHLu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt_Y_ZWA9miH"
      },
      "source": [
        "**Multiplication and comments**\n",
        "\n",
        "\n",
        "1. *Replace the '...' in line one to multiply the numerical values in line one*\n",
        "\n",
        "\n",
        "2. You can comment your code, to explain something for later reference.\n",
        "\n",
        "   *Please turn \"This is just a comment\" into a proper comment.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssxN9tka9-2o"
      },
      "source": [
        "4 ... 4\n",
        "This is just a comment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEmilTRFecFV"
      },
      "source": [
        "**Loops**\n",
        "\n",
        "A for loop is used to repeat a block of code multiple times. It is common to use the for loop when the number of iterations is fixed (e.g. iterating over a fixed list of items in a shopping list), but there are other loops as well.\n",
        "\n",
        "*Change the code below to print all elements of your list:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrSVVOz2Uzm9"
      },
      "source": [
        "your_list = ['a','b','c','d']\n",
        "\n",
        "for ... ... ... :\n",
        "  print (...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOEUQc1SVzqh"
      },
      "source": [
        "*Please change the ... to correct the code so that the loop is broken after the loop reaches the 'c' in your list.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYHdF1AVlZYy"
      },
      "source": [
        "for item ... your_list:\n",
        "  print(item)\n",
        "  if item ... ...:\n",
        "    print (\"Breaking\")\n",
        "    ...\n",
        "\n",
        "print(\"Finished\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_wm9FcxVd8T"
      },
      "source": [
        "**Pandas**\n",
        "\n",
        "pandas is a program library for the Python programming language that provides tools for data management and analysis. In particular, it contains data structures and operators for accessing tables and time series.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMs0xplyHutE"
      },
      "source": [
        "*Please import the pandas package as pd:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvlvkfJvH8NK"
      },
      "source": [
        "import ... as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create a numpy 4x4 numpy matrix with random integer values\n",
        "myarr = (np. ... (4,4)*10). ... .astype(int)\n",
        "\n",
        "df = ...(myarr)\n",
        "print(df)"
      ],
      "metadata": {
        "id": "C6rhpHpAcaRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the dataframe using the Loc function to return all numbers that are larger than 3\n",
        "df_larger =\n",
        "print(df_larger)\n",
        "\n",
        "# Delete all rows, where the values are smaller than 5\n",
        "df_delete =\n",
        "print(df_delete)"
      ],
      "metadata": {
        "id": "A-kAlor4e62Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky94Fe9pVu8a"
      },
      "source": [
        "## Basic Machine Learning: Linear Regression using Sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KADPUVuDC1w"
      },
      "source": [
        "There are **five basic steps** when you’re implementing linear regression:\n",
        "\n",
        "1. Import the packages and classes you need.\n",
        "2. Provide data to work with and eventually do appropriate transformations.\n",
        "3. Create a regression model and fit it with existing data.\n",
        "4. Check the results of model fitting to know whether the model is satisfactory.\n",
        "5. Apply the model for predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vxgqggvDgEG"
      },
      "source": [
        "**Step 1: Import packages and classes**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4w2k0VNDzkd"
      },
      "source": [
        "*Please import the package numpy and the class LinearRegression:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOXsjNrzDqTL"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25zDeGGiEBUw"
      },
      "source": [
        "The fundamental data type of NumPy is the array type called numpy.ndarray. From now on, we're using the term array to refer to instances of the type numpy.ndarray."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qNh58vEEMWL"
      },
      "source": [
        "**Step 2: Provide data:**\n",
        "\n",
        "Here we have some vectors as data for our regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfhGOPWEuI72"
      },
      "source": [
        "X = np.array([-0.51,  0.67, -0.47,  0.6 ,  1.71, -0.16,  0.34, -1.02, -0.68,\n",
        "         2.43, -1.2 ,  0.51, -0.91,  0.66, -0.53, -2.31, -0.65,  1.34,\n",
        "         0.11,  1.1 ,  1.52, -0.96, -1.29, -0.71, -0.3 ,  1.46, -1.08,\n",
        "         0.24, -0.  ,  0.28,  0.9 ,  0.75, -0.62, -0.71,  1.56, -0.33,\n",
        "         0.14, -1.43, -0.66,  0.2 ])\n",
        "y = np.array([-0.46,  0.63, -0.3 ,  0.66,  1.62, -0.55,  0.27, -1.03, -0.68,\n",
        "         2.25, -1.25,  0.47, -0.51,  0.63, -0.62, -2.42, -0.76,  1.44,\n",
        "        -0.12,  1.38,  1.6 , -1.22, -1.21, -0.63, -0.37,  1.24, -1.18,\n",
        "         0.37,  0.17,  0.4 ,  0.88,  0.77, -0.49, -0.44,  1.63, -0.37,\n",
        "         0.02, -1.36, -0.51,  0.07])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sgdxd-cp4VBx"
      },
      "source": [
        "*Please print the variables X and y to check if they are correct:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpgJWagDE-x9"
      },
      "source": [
        "...\n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6PcWKhwFML5"
      },
      "source": [
        "**Step 3: Create a model and fit it**\n",
        "\n",
        "The next step is to create a linear regression model and fit it using the existing data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk6SGwfuFihF"
      },
      "source": [
        "*Please create and fit the model (fitting means, sklearn will estimate your parameters for a given Input using the specified model):*\n",
        "**Attention: Many times, sklearn needs input of a specific shape. If this is the case (as it will be here), you may be required to reshape data, even if is basically the same, e.g. reshaping a (n,) array to a (n,1) array.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5HYk7zYICma"
      },
      "source": [
        "model = LinearRegression() #..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYmtW_4dFwfi"
      },
      "source": [
        "**Step 4: Get results**\n",
        "\n",
        "Once you have your model fitted, you can get the results to check whether the model works satisfactorily and interpret it.\n",
        "\n",
        "*Please obtain the coefficient of determination (𝑅²):*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owg_cgsgF7Uf"
      },
      "source": [
        "r_sq = model.fit(X.reshape(-1, 1), y) #...\n",
        "print('coefficient of determination:', model.score(X.reshape(-1, 1), y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7IOBX0JGG9U"
      },
      "source": [
        "*Now, please also take a look at the estimated parameters for the intercept and the slope:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t08j85rGJiP"
      },
      "source": [
        "print('intercept:', ...)\n",
        "\n",
        "print('slope:', ...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKQtcJTsGRSO"
      },
      "source": [
        "The code above illustrates how to get 𝑏₀ and 𝑏₁. You may notice that the intercept is a scalar, while the slope is an array, as you can have multiple independent variables (X)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQbgOa03Gcjq"
      },
      "source": [
        "**Step 5: Predict response**\n",
        "\n",
        "Once there is a satisfactory model, you can use it for predictions with either existing or new data.\n",
        "\n",
        "*To obtain the predicted response for X_scaled, change the following code:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn_1GeBTI1xm"
      },
      "source": [
        "y_pred = ...\n",
        "print('predicted response:', y_pred , sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmP1TirNI7UT"
      },
      "source": [
        "*Let's plot both original data and our predicted values as a line:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_zYLgzOJCCe"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(...)\n",
        "plt.plot(...)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nj0pBvAEGyOU"
      },
      "source": [
        "In practice, regression models are often applied for forecasts.\n",
        "\n",
        "*Try to use the model to forecast a y value for a single random X value:*\n",
        "**Attention: Make sure, you provide your data in the correct format expected by your model - this can not be an integer ;)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5epV7S-MZsF"
      },
      "source": [
        "X = ...\n",
        "y_new = ...\n",
        "print(y_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmI6Szxaid4z"
      },
      "source": [
        "## Machine Learning Pipeline: Random Forest with Sklearn\n",
        "\n",
        "We will now execute a Machine Learning pipeline including data preparation, model training and performance evaluation using a Random Forest and a Linear Regression as a baseline model.\n",
        "\n",
        "We will use a dataset, where customers rated the quality of wine (For details, see: https://archive.ics.uci.edu/ml/datasets/Wine).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWmk2uMXYoRB"
      },
      "source": [
        "**Step 1: Data Preparation**\n",
        "\n",
        "\n",
        "*Please load the two datasets properly, and store them in two dataframes:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COtKM6dDRd5u"
      },
      "source": [
        "import pandas as pd\n",
        "url_white = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'\n",
        "url_red = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
        "\n",
        "df_white = ...\n",
        "df_red = ...\n",
        "\n",
        "print(df_white.shape,df_red.shape) # does this look resonable? Make sure you have the correct number of columns. If necessary, take a look at your data!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atoUpOnrMum_"
      },
      "source": [
        "*Now add a column \"color\" with the wine colors white = 0 or red = 1 respectively (to preserve this information) and combine both dataframes into one:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MK9qxcwYMtuP"
      },
      "source": [
        "... # add color column to df_white\n",
        "... # add color column to df_red\n",
        "df_wines = ... # combine the two dataframes in a single long table\n",
        "\n",
        "print(df_wines.shape) # this should be (6497, 13)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsIP2kuhMA68"
      },
      "source": [
        "Remember, we want to understand **what drives customer's quality perception for wines**. To accomplish that, we want to predict the perceived quality of a wine (our dependent variable, y) using all other available variables (indepentend variables, X).\n",
        "\n",
        "*Let's take a brief look at your dependend variable - how many wines per quality rating are there?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv32nSUxVLTR"
      },
      "source": [
        "... # create a table with the number of occurences per quality value (e.g., there should be 193 wines rated 8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGgViSRJjy1s"
      },
      "source": [
        "*Now split the data in dependent (y) - quality - and independent(X) variable:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOIyY8IkMg3R"
      },
      "source": [
        "X,y = ...\n",
        "\n",
        "print(y.head(3))\n",
        "X.head(3) # Does this look good?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCcOLK-ZMkUI"
      },
      "source": [
        "**Step 2: Training**\n",
        "\n",
        "*Let's split the dataset so that 80% of the data is used to train the model and 20% is kept for future evaluation:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1LDvrZ5MorA"
      },
      "source": [
        "from sklearn.model_selection import ...\n",
        "X_train, X_test, y_train, y_test = ...\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FVKFEvSMtzc"
      },
      "source": [
        "Now we can define and fit our model on the training dataset. Let's do this for both a random forest and a linear regression model. Using two model will allow us to better evaluate our model performance in the end.\n",
        "\n",
        "*Please fit a linear regression and a random forest model:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q31IBY8OMw7k"
      },
      "source": [
        "from sklearn.ensemble import ... # take suitable random forest module\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model_rfo = ... # initialize random rorest model\n",
        "model_lin = ... # initialize linear regression model\n",
        "\n",
        "model_rfo.fit(...)\n",
        "model_lin.fit(...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTwMheqMM0Kf"
      },
      "source": [
        "**Step 3: Performance Evaluation**\n",
        "\n",
        "*Now use the fitted model to evaluate performance on your 20% holdout test dataset, using the mean squared error (*MSE*) performance metric:*\n",
        "Note, that neither model had 'seen' this data before. This is crucial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E6JDqYOM7AR"
      },
      "source": [
        "from sklearn.metrics import ... # please use MSE\n",
        "\n",
        "y_hat_rfo = ... # make predictions using your random forest model\n",
        "y_hat_lin = ... # make predictions using your linear regression model\n",
        "\n",
        "mse_rfo = ... # calculate random forest prediction MSE\n",
        "mse_lin = ... # calculate linear regression prediction MSE\n",
        "\n",
        "print('MSE Random Forest: %.3f' % mse_rfo)\n",
        "print('MSE Linear Regression: %.3f' % mse_lin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHDkZUZMWP-B"
      },
      "source": [
        "<img src =\"https://i.ibb.co/Yyk6PBP/laughing-men-testing.jpg\" width=500 height=300>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyUb34dPpqdk"
      },
      "source": [
        "*It is always a good idea, to get a visual intuition as well, so let's quickly plot ground truth y_test against our predictions y_hat_rfo and y_hat_lin*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FtycYu3cdE-"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "... # generate a scatter plot for your random forest predictions\n",
        "plt.show()\n",
        "\n",
        "... # generate a scatter plot for your linear regression predictions\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CN3utEwql_5"
      },
      "source": [
        "Taking both the MSE and the visual representation - which model performs better?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK9El9w1qv1v"
      },
      "source": [
        "# write your answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QED3CFGxq1Wi"
      },
      "source": [
        "**Step 4: Generating Insights**\n",
        "\n",
        "Let's use our model to find out what is the most important variable for quality.\n",
        "\n",
        "*Please create a dataframe which has all the X variables and their respective performance and sort the dataframe by importance:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pacpzzrhd1fe"
      },
      "source": [
        "insights = pd.DataFrame(columns=['variable','importance'])\n",
        "\n",
        "insights['variable'] = ...\n",
        "insights['importance'] = ...\n",
        "\n",
        "insights. ... # please print the sorted table - what is most imporant?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced Machine Learning Pipeline 1: Language Models using Huggingface\n",
        "\n",
        "Finally we will now analyze the sentiment of thousands of tweets. The dataset we use includes english tweets about how good or bad flying with a specific airline was. We will use these tweets to predict, if the content of the tweet is positive, neutral or negative.\n",
        "\n",
        "Because unstructured data in form of text is really hard to analyze for machine learning models we will now use a modern neural network. The neural net we will use is already trained to understand english language. In the upcoming cells we will download the model and train it (named fine-tuning) on the task of sentiment classification (e.g. predicting if the tweet is positive, neutral or negative in sentiment).\n",
        "\n",
        "We will use code from the website huggingface.com. A website that hosts many of the latest trained neural networks. By starting with an already pre-trained network we reduce the amount of work that we need to do by a lot.\n",
        "Many researchers today use such pre-trained models to then later train them to classify text, analyze images or other interesting data.\n",
        "\n",
        "**Please fill in all the ... in the upcoming cells to make your model work.** If you ever need more information feel free to google about the code or look specifically into the documentation of the huggingface code we will be using. Link: https://huggingface.co/docs"
      ],
      "metadata": {
        "id": "uEOBP9nzFwwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we install the transformer library (made by the huggingface team) that we need and import all needed packages"
      ],
      "metadata": {
        "id": "Sjp7SIepFUTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet transformers torch datasets accelerate"
      ],
      "metadata": {
        "id": "81I8SeZ1F56t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer,  AutoModelForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "z-TSr7ALGB8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To work with large language models like we are now doing, you need a graphics card (GPU). Run the next cell. If it outputs 'True', you are ready to continue.\n",
        "If the next cell outputs 'False', you need to activate your GPU in Google Colab.\n",
        "\n",
        "For that:\n",
        "\n",
        "1. Go to Menu at the top (Menü oben) > Runtime (Laufzeit) > Change runtime (Laufzeittyp ändern).\n",
        "2. Change hardware acceleration to GPU (Hardwarebeschleuniger zu GPU)."
      ],
      "metadata": {
        "id": "8AeSIat2vX1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "qKJhtcQ4vGw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Data Preparation**"
      ],
      "metadata": {
        "id": "R1EUjacuGWwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"dair-ai/emotion\")"
      ],
      "metadata": {
        "id": "qWgGS0QdmO-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(dataset['train'])"
      ],
      "metadata": {
        "id": "AV7UpZsCmanb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now look at the first 5 rows of data in your dataframe. Is there a text and label column included?\n",
        "\n",
        "Correct labels for the emotions should be: sadness (0), joy (1), love (2), anger (3), fear (4), surprise (5)."
      ],
      "metadata": {
        "id": "Bo6PTp9qm8H9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "... # Display the first five rows of the train dataframe"
      ],
      "metadata": {
        "id": "bP3ctBUNyfo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Loading model**\n",
        "\n",
        "Next we load a model from the Huggingface Libary. We use a model that is already trained to understand english. If you want to, you can try out other models from the website by copying in the model name into the variable below: https://huggingface.co/models."
      ],
      "metadata": {
        "id": "k4wBlql4GaC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"distilbert-base-uncased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6).to(\"cuda\")"
      ],
      "metadata": {
        "id": "fmENhUoHGh4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To further work with our model, we define a function to tokenize the text:"
      ],
      "metadata": {
        "id": "xHzoDeoDEmvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "  return tokenizer(example['text'], truncation=True)"
      ],
      "metadata": {
        "id": "wj2ZA6UIolpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "h36fDQ6vnXh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "def custom_data_collator(features):\n",
        "    data_collator = DataCollatorWithPadding(tokenizer)\n",
        "    batch = data_collator(features)\n",
        "    return batch"
      ],
      "metadata": {
        "id": "tgSjkXfhoyh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Training the model**"
      ],
      "metadata": {
        "id": "VKIu6bKqHIRM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets train the model. First we define the hyperparameters. These define for example how long the model should be trained or how many tweets the model should be trained on at each point in time.\n",
        "\n",
        "Try to choose a good numberical value as the number of epochs (iterations the model will see all of your data) and batch sizes (number of examples the model is shown in each step).\n",
        "\n",
        "**Keep in mind: If you choose the batch size too large, the colab session likely to crash**"
      ],
      "metadata": {
        "id": "FQ0gosicKhHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    num_train_epochs=..., # choose the number of training epochs\n",
        "    per_device_train_batch_size=..., #..., # set the batch size to a good value\n",
        "    per_device_eval_batch_size=..., #..., # set this batch size to same you chose above\n",
        "    load_best_model_at_end= True,\n",
        "    logging_steps=25,\n",
        "    save_steps=25,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    output_dir=\"/content/model\"\n",
        ")"
      ],
      "metadata": {
        "id": "sdpJXfusHTcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make the training work, the trainer must know if the model is getting better or not learning. Therefore here we define a metric for the trainer to use. Because we are doing classification we choose the accuracy score again."
      ],
      "metadata": {
        "id": "5pKzzl2FL5ul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ... # please import the accuracy score metric from sklearn\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    predictions = pred.predictions.argmax(-1)\n",
        "    acc = ...(labels, predictions)  # here change this to the metric you imported\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "    }"
      ],
      "metadata": {
        "id": "aAMP4LEgrus3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we define the trainer. The trainer is fitting the model parameters and minimizing the loss function to make the prediction as accurate as possible. You just need to define the trainer and afterwards start the training process. Then the model will get trained and we can use it to predict new data afterwards."
      ],
      "metadata": {
        "id": "o0Y5gz_SNKVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model, #..., # put in the model that we loaded a few cells ago\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset['train'],\n",
        "    eval_dataset=tokenized_dataset['validation'],\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=custom_data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "uCuvfxLQHXvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the model is training. After every couple of steps it does report how well it learned. Look at the values for training, validation loss as well as accuracy. What do you notice by looking at the values? Go they all go down or up?"
      ],
      "metadata": {
        "id": "zzj4Txv8tl1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer. ... # train the model"
      ],
      "metadata": {
        "id": "OfxqrMW3HbwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Evaluation and prediction**"
      ],
      "metadata": {
        "id": "EpYFuducHKow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now after we trained the model lets evaluate the model by printing out some technical metrics."
      ],
      "metadata": {
        "id": "dRY0D8GVNZDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "... # use a built-in method to evaluate your trained model"
      ],
      "metadata": {
        "id": "yprnhpzmHfyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lastly we can try out if the model really works. You dont need to change anything in the cell below. Just enter any sentence in the function 2 cells below."
      ],
      "metadata": {
        "id": "O6PMlj2tNk8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prediction(text):\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, max_length=64, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model(**inputs)\n",
        "    probs = outputs[0].softmax(1)\n",
        "    classes = {0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear', 5: 'surprise'}\n",
        "    predicted_category = classes[int(torch.argmax(probs))]\n",
        "    return predicted_category"
      ],
      "metadata": {
        "id": "cNC5x7ApHlVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now test how good your model is. Try out any text to see if your model predicts the sentiment correctly.. and is your model good?"
      ],
      "metadata": {
        "id": "j02DJ4wNNuI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_prediction(...) # Come up with a sentence try your newly built model"
      ],
      "metadata": {
        "id": "9DF72LcqCszn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save your model so that you can use it in the future."
      ],
      "metadata": {
        "id": "Ol8zC5qqN357"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model. ... # save your model"
      ],
      "metadata": {
        "id": "5Zb-RQpct2IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced Machine Learning Pipeline 2: Generative AI"
      ],
      "metadata": {
        "id": "sH0r8GM0pp5I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load a generative AI model (similar to ChatGPT etc.) look through the models on huggingface to find a fitting model. Load a model that is able to generate text. It should have a maximum of 2 billion parameters (2B).\n",
        "\n",
        "The parameters are most of the time directly mentioned in the name of the model. If you try to load a model that has much more parameters, the memory of the colab system you are running might not be enough. Then you would get an Cuda: OutOfMemory error or the session would crash.\n",
        "\n",
        "**Hint: Look on https://huggingface.co/models to search for models that can generate texts.**\n"
      ],
      "metadata": {
        "id": "A2XJDw4KpydR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "pipe = pipeline('text-generation', model=..., cuda=0)"
      ],
      "metadata": {
        "id": "OEbTuLrnpvaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now generate any text you want. These models are similar to ChatGPT, so just ask any kind of question or write whatever you want.\n",
        "\n",
        "In this way you can use text generative models directly in Python instead of using a website to access their functionalities.\n",
        "\n",
        "*There is no right or wrong answer here, we just want to see that you are able to call the model pipeline correctly to get any kind of output from the model.*"
      ],
      "metadata": {
        "id": "6bcJtYRvsJQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer = pipe(...)\n",
        "print(answer[0])"
      ],
      "metadata": {
        "id": "cxnXbhUbqJcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced Machine Learning Pipeline 3: Image classification"
      ],
      "metadata": {
        "id": "5mg0B_emt5i6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part we will use a model to get some image classification. The dataset we look at contains different pictures of forests/landscapes. Some of them show wildfire and some show landscapes which are not currently burning. We want to later use a model to tell if any picture we feed in shows scenes of a wildfire or not.\n",
        "\n",
        "Therefore we will call an image classification model and train it on our dataset.\n",
        "\n",
        "We will use code from the website huggingface.com. A website that hosts many of the latest image handling models. By starting with an already pre-trained model we reduce the amount of work that we need to do by a lot. Many researchers today use such pre-trained models to then later train them to classify specific use cases."
      ],
      "metadata": {
        "id": "PHhuJrF7E-Az"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we install the corresponding tools we need later on."
      ],
      "metadata": {
        "id": "4ieZ6LVMGrRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate datasets --quiet\n",
        "\n",
        "from transformers import ViTForImageClassification\n",
        "import numpy as np\n",
        "from datasets import load_metric"
      ],
      "metadata": {
        "id": "_FP9A_b03Wg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just run this cell. Here we download a dataset from our github profile as a zip folder and unpack it using python."
      ],
      "metadata": {
        "id": "jD8Js2iwuFIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "\n",
        "url = 'https://github.com/Maximilianwte/Data-Science-for-Customer-Insight/raw/main/image_data.zip'\n",
        "response = requests.get(url, stream=True)\n",
        "with open('image_data.zip', 'wb') as out_file:\n",
        "    out_file.write(response.content)\n",
        "\n",
        "with zipfile.ZipFile('/content/image_data.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('images')"
      ],
      "metadata": {
        "id": "ooz7mgp5kn-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here use the function we imported to load the dataset from the folder into the Huggingface dataset class."
      ],
      "metadata": {
        "id": "TRYRLqM3uPum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "dataset = ... (\"imagefolder\", data_dir=\"/content/images/image_data\")"
      ],
      "metadata": {
        "id": "WzPwg8E7j3MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTFeatureExtractor\n",
        "import torch\n",
        "\n",
        "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name_or_path)"
      ],
      "metadata": {
        "id": "gSgM6TZZj3J6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this cell we convert all images into numerical values (embeddings). Please return the complete variable to make the function work."
      ],
      "metadata": {
        "id": "LlMIMzlbup9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform(example_batch):\n",
        "    inputs = feature_extractor([x for x in example_batch['image']], return_tensors='pt')\n",
        "    inputs['labels'] = example_batch['label']\n",
        "    return ... # return the variable that we changed in the 2 lines above\n",
        "\n",
        "prepared_ds = dataset.with_transform(transform)"
      ],
      "metadata": {
        "id": "CP0wetZQj3Hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define further model functions:"
      ],
      "metadata": {
        "id": "9kxWxYPtGohN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
        "        'labels': torch.tensor([x['labels'] for x in batch])\n",
        "    }"
      ],
      "metadata": {
        "id": "n9wWOMboj3B2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make use of a metric which can help evaluating the model."
      ],
      "metadata": {
        "id": "mJaXwJTWGsgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = load_metric(\"accuracy\")\n",
        "def compute_metrics(p):\n",
        "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\n",
        "\n",
        "labels = dataset['train'].features['label'].names\n",
        "\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    num_labels=len(labels),\n",
        "    id2label={str(i): c for i, c in enumerate(labels)},\n",
        "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
        ").to('cuda')\n"
      ],
      "metadata": {
        "id": "BR4_ozplkFLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define and train the Trainer we will use to classfiy the images."
      ],
      "metadata": {
        "id": "VezPS6HxGzNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "from transformers import Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"./vit-model\",\n",
        "  per_device_train_batch_size=12,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=1,\n",
        "  fp16=True,\n",
        "  save_steps=30,\n",
        "  eval_steps=30,\n",
        "  logging_steps=10,\n",
        "  learning_rate=2e-4,\n",
        "  save_total_limit=2,\n",
        "  remove_unused_columns=False,\n",
        "  push_to_hub=False,\n",
        "  report_to='tensorboard',\n",
        "  load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=prepared_ds['train'],\n",
        "    eval_dataset=prepared_ds['test'],\n",
        "    tokenizer=feature_extractor\n",
        ")\n",
        "\n",
        "train_results = trainer.train()"
      ],
      "metadata": {
        "id": "LaAUy3hRkIq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use some metrics to evaluate our training process."
      ],
      "metadata": {
        "id": "I05M9mSmHB73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = trainer.evaluate(prepared_ds['test'])\n",
        "trainer.log_metrics(\"eval\", metrics)\n",
        "\n"
      ],
      "metadata": {
        "id": "WSObKFzRmgfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get an image processor to feed in data to our model."
      ],
      "metadata": {
        "id": "iFcsSpDKHdM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from transformers import AutoImageProcessor\n",
        "\n",
        "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
        "image_processor = AutoImageProcessor.from_pretrained(model_name_or_path)\n"
      ],
      "metadata": {
        "id": "RA5-J8bNm-kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the prediction function."
      ],
      "metadata": {
        "id": "76iTC_NHHhTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from IPython.display import display\n",
        "\n",
        "def predict_url(url):\n",
        "  response = requests.get(url)\n",
        "  img = Image.open(BytesIO(response.content)).resize((500, 400))\n",
        "  display(img)\n",
        "  inputs = image_processor(images=img, return_tensors=\"pt\").to('cuda')\n",
        "  outputs = model(**inputs)\n",
        "  predicted_class_id = outputs.logits.argmax(-1).item()\n",
        "  predicted_class_label = model.config.id2label[str(predicted_class_id)]\n",
        "  return predicted_class_label\n"
      ],
      "metadata": {
        "id": "TecdurlAnPBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Browse the web for some pictures (wildfire, no wildfire,...) and try to get a prediction. Is our model any good?"
      ],
      "metadata": {
        "id": "zpf9nSEEHlwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict_url(\"https://s.hdnux.com/photos/01/34/12/77/24162255/5/1200x0.jpg\")"
      ],
      "metadata": {
        "id": "k9mJF9jhnDNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_url(\"https://i.insider.com/5eb992a7e3c3fb1d85133fc5?width=700\")"
      ],
      "metadata": {
        "id": "QEdQzsM7puV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tClVksnmXsNn"
      },
      "source": [
        "## Save & Submit\n",
        "\n",
        "Please make sure you included your name and matrikel at the top of this notebook, save your work **with your name in the file name**, and send **two** files to maximilian.witte@uni-hamburg.de:\n",
        "- Ipynb *(Go to File -> Download .ipynb)*\n",
        "- PDF *(Go to File -> Print -> Save as PDF)* (optional)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DPboysUljWFD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}